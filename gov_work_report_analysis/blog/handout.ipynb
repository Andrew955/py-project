{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分析了六十多年间100万字的政府工作报告，我看到了这样的变迁..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每年我国政府都会发布年度政府工作报告，而报告中出现最多的TopN关键词都会成为媒体热议的焦点，更是体现了过去一年和未来政府工作的重点和趋势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在中央政府网站上也可以看到从1954年至今每年的政府工作报告，链接：http://www.gov.cn/guoqing/2006-02/16/content_2616810.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么突发奇想，从这60多年间的政府工作报告中可以看出来什么样的变迁呢？说干就干，下面就是实现这一想法的历程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目标是什么"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 获取1954年至今历年政府工作报告的全文，并统计出每年政府工作报告中Top20的关键词，并用图表可视化展示出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 统计每十年的政府工作报告的合并Top20关键词，并用图表直观展示出来，从中分析出变迁的趋势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 准备工作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据获取阶段需要有两个准备："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 网页链接：\n",
    "\n",
    "2017年政府工作报告链接：http://www.gov.cn/premier/2017-03/16/content_5177940.htm\n",
    "\n",
    "1954~2017年政府工作报告汇总页面链接：http://www.gov.cn/guoqing/2006-02/16/content_2616810.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 技术准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用非常好用的web库——requests获取网页内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用BeautifulSoup库解析网页HTML内容，从中解析出政府工作报告的文本内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据处理与分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用结巴分词库（jieba）对政府工作报告文本内容进行分词、过滤无效词、统计词频。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用matplotlib库画出每十年政府工作报告关键词的散点分布图，通过对比不同年代的图，分析其中的变化趋势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 动手搞"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "准备工作做好后，我们开始按照计划一步步地开始实施。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取网页HTML内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了代码复用，创建一个html_utils.py文件，提供下载网页内容的函数，并提供了一个HTML页面解析异常类："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# coding:utf-8\n",
    "# html工具函数\n",
    "import requests\n",
    "\n",
    "# 通用请求Hearders\n",
    "HEADERS = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.78 Safari/537.36'}\n",
    "\n",
    "# html页面解析异常\n",
    "class HtmlParseError(Exception):\n",
    "    def __init__(self,value):\n",
    "        self.value = value\n",
    "        \n",
    "    def __str__(self):\n",
    "        return repr(self.value)\n",
    "\n",
    "# 获取网页页面html全文\n",
    "def get_html(url):\n",
    "    resp = requests.get(url,headers = HEADERS)\n",
    "    resp.encoding = 'utf-8'\n",
    "    if resp:\n",
    "        return resp.text\n",
    "    return None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个分词工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的总体思路是先获取网页内容，然后从网页内容中解析出政府工作报告正文，然后对其进行分词，这里分词需要用到jieba模块，我们创建一个cut_text_utils.py文件，在其中提供分词的函数，内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# coding:utf-8\n",
    "# 分词操作工具函数\n",
    "import sys\n",
    "import jieba\n",
    "from collections import Counter\n",
    "\n",
    "# 对一段文本进行分词，并过滤掉长度小于2的词（标点、虚词等），用全模式分词\n",
    "def cut_text(text):\n",
    "    cut_list = jieba.cut(text.lower())\n",
    "    return [word for word in cut_list if len(word) >= 2]\n",
    "    \n",
    "# 统计出一段文本中出现数量最多的前n个关键词及数量\n",
    "def get_topn_words(text,topn):\n",
    "    cut_list = cut_text(text)\n",
    "    counter = Counter(cut_list)\n",
    "    return counter.most_common(topn)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 设置字节流编码方式为utf-8\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding('utf-8')\n",
    "    \n",
    "    s = u'我想和女朋友一起去北京故宫博物院参观和闲逛。'\n",
    "    # print cut_text(s)\n",
    "    print get_topn_words(s,5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述Demo脚本，输出："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(u'参观', 1), (u'北京故宫博物院', 1), (u'一起', 1), (u'女朋友', 1), (u'闲逛', 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建一个绘图工具"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终要使用matplotlib库绘出关键词的散点图，可以更直观地进行分析，所以我们再写一个绘图工具文件visual_utils.py，内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# coding:utf-8\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 指定默认字体，防止画图时中文乱码\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  \n",
    "\n",
    "# 传入一组关键词及词频列表，从高到低绘出每个关键词频率的散点图\n",
    "# keywords示例：[(u'张三',10),(u'李四',12),(u'王五',7)]\n",
    "def draw_keywords_scatter(keywords,title = None,xlabel = None,ylabel = None):\n",
    "    # 先对keywords按词频从高到低排序\n",
    "    keywords = sorted(keywords,key = lambda item:item[1],reverse = True)\n",
    "\n",
    "    # 解析出关键词列表\n",
    "    words = [x[0] for x in keywords]\n",
    "    # 解析出对应的词频列表\n",
    "    times = [x[1] for x in keywords]\n",
    "    \n",
    "    x = range(len(keywords))\n",
    "    y = times\n",
    "    plt.plot(x, y, 'b^')\n",
    "    plt.xticks(x, words, rotation=45)\n",
    "    plt.margins(0.08)\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    # 图表名称\n",
    "    plt.title(title)\n",
    "    # x,y轴名称\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "def main():\n",
    "    draw_keywords_scatter([(u'张三',10),(u'李四',12),(u'王五',7)],u'出勤统计图',u'姓名',u'出勤次数')\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上面的Demo脚本，绘图结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析2017年政府工作报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们先获取到2017年的政府工作报告试试水，创建一个文件year2017_analysis.py，内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# coding:utf-8\n",
    "# 分析2017年政府工作报告，从中提取出前20个高频词\n",
    "import sys\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import html_utils\n",
    "import cut_text_utils\n",
    "\n",
    "# 2017年政府工作报告全文URL\n",
    "REPORT_URL = 'http://www.gov.cn/premier/2017-03/16/content_5177940.htm'\n",
    "\n",
    "# 从2017年政府工作报告html页面内容中解析出正文\n",
    "def parse_report_article(html):\n",
    "    soup = BS(html,'html.parser')\n",
    "    article = soup.find('div',attrs = {'id':'UCAP-CONTENT'})  # 报告正文，这里可以通过分析网页HTML结构获取到解析的方法\n",
    "    return article.text\n",
    "    \n",
    "# 传入2017年政府工作报告全文的URL，解析出topn关键词及词频\n",
    "def get_topn_words(url,topn):\n",
    "    html = html_utils.get_html(url)\n",
    "    article = parse_report_article(html)\n",
    "    return cut_text_utils.get_topn_words(article,topn)\n",
    "    \n",
    "def main():\n",
    "    # 设置字节流编码方式为utf-8\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding('utf-8')\n",
    "    \n",
    "    with open('out.tmp','w+') as fout:\n",
    "        fout.write(str(get_topn_words(REPORT_URL,20)))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述脚本，然后在当前目录下可以看到产生了一个out.tmp文件，其内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(u'发展', 125), (u'改革', 68), (u'推进', 65), (u'建设', 54), (u'经济', 52), (u'加强', 45), (u'推动', 42), (u'加快', 40), (u'政府', 39), (u'创新', 36), (u'完善', 35), (u'全面', 35), (u'企业', 35), (u'促进', 34), (u'提高', 32), (u'就业', 31), (u'实施', 31), (u'中国', 31), (u'工作', 29), (u'支持', 29)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从中可以看出2017年的前五关键词是：发展，改革，推进，建设，经济，和我们经常在媒体上看到的情况也比较吻合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析1954到2017每年的政府工作报告"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "思路是这样的，首先从汇总页面获取到每年政府工作报告网页的链接，然后分别爬取每个链接获取到网页内容，接着解析出每年的政府工作报告正文，最后对每10年的政府工作报告合并分析出Top20关键词并展示出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导包："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# coding:utf-8\n",
    "# 1954~2017年政府工作报告汇总分析\n",
    "import sys\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import html_utils\n",
    "from html_utils import HtmlParseError\n",
    "import cut_text_utils\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "汇总页面URL："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 汇总URL\n",
    "SUMMARY_URL = 'http://www.gov.cn/guoqing/2006-02/16/content_2616810.htm'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从汇总页面解析出每年政府工作报告全文页面的URL列表："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 从汇总页面解析出每年政府工作报告全文页面的URL\n",
    "# 注：只有2017年的页面URL是专题页面而非全文页面\n",
    "def get_report_urls(summary_url):\n",
    "    html = html_utils.get_html(summary_url)\n",
    "    soup = BS(html,'html.parser')\n",
    "    reports_table = soup.select('#UCAP-CONTENT table tbody')[0]\n",
    "    reports = [(atag.text,atag['href']) for trtag in reports_table.select('tr') for tdtag in trtag.select('td') if len(tdtag.select('a')) != 0 for atag in tdtag.select('a')]\n",
    "    \n",
    "    # 过滤去2017年的URL\n",
    "    report_urls = [x for x in reports if x[0] != '2017']\n",
    "    report_urls.append(('2017',REPORT2017_URL))\n",
    "    # 按照年份升序排序\n",
    "    report_urls = sorted(report_urls,key = lambda item:item[0])\n",
    "    return report_urls\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从报告正文页面html中解析出正文内容："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注：这里要考虑两种不同的页面结构进行解析。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 从报告页面html中解析出正文内容\n",
    "# 考虑到不同年份报告的2种不同的html结构，采用两种解析方案\n",
    "def parse_report_article(html):\n",
    "    soup = BS(html,'html.parser')\n",
    "    # 解析方案1\n",
    "    article = soup.select('#UCAP-CONTENT')\n",
    "    # 若article为空，则换方案2来解析\n",
    "    if len(article) == 0:\n",
    "        article = soup.select('.p1')\n",
    "        # 若还为空，则抛出异常\n",
    "        if len(article) == 0:\n",
    "            raise HtmlParseError('parse report error!')\n",
    "            \n",
    "    return article[0].text\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过上述函数结合使用，可以爬取到1954年到2017年的所有政府工作报告的文本，总字数为100万零7000多字。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着以下几个函数用来解析关键词："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 传入某一年政府工作报告全文的URL，解析出topn关键词及词频\n",
    "def get_topn_words(url,topn):\n",
    "    html = html_utils.get_html(url)\n",
    "    article = parse_report_article(html)\n",
    "    return cut_text_utils.get_topn_words(article,topn)\n",
    "\n",
    "# 传入若干个政府工作报告全文的URL，解析出合并topn关键词\n",
    "# save_reports：是否保存文本到文件中（reports.txt）\n",
    "def get_topn_words_from_urls(urls,topn,save_reports = False):\n",
    "    htmls = [html_utils.get_html(url) for url in urls]\n",
    "    # 汇总文本\n",
    "    summary_atricle = '\\n'.join([parse_report_article(html) for html in htmls])\n",
    "    if save_reports:\n",
    "        with open('reports.txt','w+') as fout:\n",
    "            fout.write(summary_atricle)\n",
    "    return cut_text_utils.get_topn_words(summary_atricle,topn)\n",
    "\n",
    "# 根据传入的每年的政府工作报告全文URL，解析出每年的topn关键词\n",
    "def get_topn_words_yearly(report_urls,topn):\n",
    "    keywords = OrderedDict()\n",
    "    # 遍历url列表，解析出每年政府工作报告的top30关键词并存入字典keywords\n",
    "    for year,url in report_urls:\n",
    "        print 'start to parse {year} report...'.format(year = year)\n",
    "        keywords[year] = get_topn_words(url,topn)\n",
    "    return keywords\n",
    "\n",
    "# 根据传入的每年的政府工作报告全文URL，解析出每个十年的合并topn关键词\n",
    "def get_topn_words_decadal(report_urls,topn):\n",
    "    # 统计出每个10年的topn关键词\n",
    "    decade1 = ['1964','1960','1959','1958','1957','1956','1955','1954']\n",
    "    decade2 = ['1987','1986','1985','1984','1983','1982','1981','1980','1979','1978','1975']\n",
    "    decade3 = ['1997','1996','1995','1994','1993','1992','1991','1990','1989','1988']\n",
    "    decade4 = ['2007','2006','2005','2004','2003','2002','2001','2000','1999','1998']\n",
    "    decade5 = ['2017','2016','2015','2014','2013','2012','2011','2010','2009','2008']\n",
    "    \n",
    "    keywords = OrderedDict()\n",
    "    decade_items = [('1954-1964',decade1),('1975-1987',decade2),('1988-1997',decade3),('1998-2007',decade4),('2008-2017',decade5)]\n",
    "    for years,decade in decade_items:\n",
    "        print 'start to parse {years} reports...'.format(years = years)\n",
    "        urls = [item[1] for item in report_urls if item[0] in decade]\n",
    "        keywords[years] = get_topn_words_from_urls(urls,topn)\n",
    "        \n",
    "    return keywords\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "汇总以上代码，合并为summary_analysis.py文件，内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# coding:utf-8\n",
    "# 1954~2017年政府工作报告汇总分析\n",
    "import sys\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import html_utils\n",
    "from html_utils import HtmlParseError\n",
    "import cut_text_utils\n",
    "import visual_utils\n",
    "\n",
    "# 汇总URL\n",
    "SUMMARY_URL = 'http://www.gov.cn/guoqing/2006-02/16/content_2616810.htm'\n",
    "\n",
    "# 2017年政府工作报告全文URL\n",
    "REPORT2017_URL = 'http://www.gov.cn/premier/2017-03/16/content_5177940.htm'\n",
    "\n",
    "# 从汇总页面解析出每年政府工作报告全文页面的URL\n",
    "# 注：只有2017年的页面URL是专题页面而非全文页面\n",
    "def get_report_urls(summary_url):\n",
    "    html = html_utils.get_html(summary_url)\n",
    "    soup = BS(html,'html.parser')\n",
    "    reports_table = soup.select('#UCAP-CONTENT table tbody')[0]\n",
    "    reports = [(atag.text,atag['href']) for trtag in reports_table.select('tr') for tdtag in trtag.select('td') if len(tdtag.select('a')) != 0 for atag in tdtag.select('a')]\n",
    "    \n",
    "    # 过滤去2017年的URL\n",
    "    report_urls = [x for x in reports if x[0] != '2017']\n",
    "    report_urls.append(('2017',REPORT2017_URL))\n",
    "    # 按照年份升序排序\n",
    "    report_urls = sorted(report_urls,key = lambda item:item[0])\n",
    "    return report_urls\n",
    "\n",
    "# 从报告页面html中解析出正文内容\n",
    "# 考虑到不同年份报告的2种不同的html结构，采用两种解析方案\n",
    "def parse_report_article(html):\n",
    "    soup = BS(html,'html.parser')\n",
    "    # 解析方案1\n",
    "    article = soup.select('#UCAP-CONTENT')\n",
    "    # 若article为空，则换方案2来解析\n",
    "    if len(article) == 0:\n",
    "        article = soup.select('.p1')\n",
    "        # 若还为空，则抛出异常\n",
    "        if len(article) == 0:\n",
    "            raise HtmlParseError('parse report error!')\n",
    "            \n",
    "    return article[0].text\n",
    "\n",
    "# 传入某一年政府工作报告全文的URL，解析出topn关键词及词频\n",
    "def get_topn_words(url,topn):\n",
    "    html = html_utils.get_html(url)\n",
    "    article = parse_report_article(html)\n",
    "    return cut_text_utils.get_topn_words(article,topn)\n",
    "\n",
    "# 传入若干个政府工作报告全文的URL，解析出合并topn关键词\n",
    "# save_reports：是否保存文本到文件中（reports.txt）\n",
    "def get_topn_words_from_urls(urls,topn,save_reports = False):\n",
    "    htmls = [html_utils.get_html(url) for url in urls]\n",
    "    # 汇总文本\n",
    "    summary_atricle = '\\n'.join([parse_report_article(html) for html in htmls])\n",
    "    if save_reports:\n",
    "        with open('reports.txt','w+') as fout:\n",
    "            fout.write(summary_atricle)\n",
    "    return cut_text_utils.get_topn_words(summary_atricle,topn)\n",
    "\n",
    "# 根据传入的每年的政府工作报告全文URL，解析出每年的topn关键词\n",
    "def get_topn_words_yearly(report_urls,topn):\n",
    "    keywords = OrderedDict()\n",
    "    # 遍历url列表，解析出每年政府工作报告的top30关键词并存入字典keywords\n",
    "    for year,url in report_urls:\n",
    "        print 'start to parse {year} report...'.format(year = year)\n",
    "        keywords[year] = get_topn_words(url,topn)\n",
    "    return keywords\n",
    "\n",
    "# 根据传入的每年的政府工作报告全文URL，解析出每个十年的合并topn关键词\n",
    "def get_topn_words_decadal(report_urls,topn):\n",
    "    # 统计出每个10年的topn关键词\n",
    "    decade1 = ['1964','1960','1959','1958','1957','1956','1955','1954']\n",
    "    decade2 = ['1987','1986','1985','1984','1983','1982','1981','1980','1979','1978','1975']\n",
    "    decade3 = ['1997','1996','1995','1994','1993','1992','1991','1990','1989','1988']\n",
    "    decade4 = ['2007','2006','2005','2004','2003','2002','2001','2000','1999','1998']\n",
    "    decade5 = ['2017','2016','2015','2014','2013','2012','2011','2010','2009','2008']\n",
    "    \n",
    "    keywords = OrderedDict()\n",
    "    decade_items = [('1954-1964',decade1),('1975-1987',decade2),('1988-1997',decade3),('1998-2007',decade4),('2008-2017',decade5)]\n",
    "    for years,decade in decade_items:\n",
    "        print 'start to parse {years} reports...'.format(years = years)\n",
    "        urls = [item[1] for item in report_urls if item[0] in decade]\n",
    "        keywords[years] = get_topn_words_from_urls(urls,topn)\n",
    "        \n",
    "    return keywords\n",
    "\n",
    "def main():\n",
    "    # 设置字节流编码方式为utf-8\n",
    "    reload(sys)\n",
    "    sys.setdefaultencoding('utf-8')\n",
    "    \n",
    "    # 按年代分析每10年的政府工作报告\n",
    "    report_urls = get_report_urls(SUMMARY_URL)\n",
    "    keywords = get_topn_words_decadal(report_urls,20)\n",
    "    \n",
    "    # 将结果保存到文件\n",
    "    with open('out.tmp','w+') as fout:\n",
    "        for years,words in keywords.items():\n",
    "            fout.write('【{years}】\\n'.format(years = years.decode('unicode-escape').encode('utf-8')))\n",
    "            for word,count in words:\n",
    "                fout.write('{word}:{count};'.format(word = word,count = count))\n",
    "            fout.write('\\n\\n')\n",
    "            \n",
    "    # 绘出散点图\n",
    "    for years,words in keywords.items():\n",
    "        visual_utils.draw_keywords_scatter(words[:20],u'{years}年政府工作报告关键词Top{topn}'.format(years = years,topn = 20),u'关键词',u'出现总次数')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行该文件，在当前目录下的out.tmp文件可以看到其内容如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【1954-1964】\n",
    "我们:932;人民:695;国家:690;我国:664;建设:650;发展:641;社会主义:618;生产:509;工业:491;农业:481;工作:396;增长:385;增加:376;必须:361;计划:339;已经:328;方面:299;进行:298;全国:295;企业:267;\n",
    "\n",
    "【1975-1987】\n",
    "发展:1012;我们:1011;经济:875;建设:664;我国:609;企业:586;人民:577;国家:569;社会主义:535;改革:499;工作:488;生产:486;必须:451;提高:368;增长:349;方面:349;进行:349;问题:320;增加:290;加强:288;\n",
    "\n",
    "【1988-1997】\n",
    "发展:1182;经济:789;建设:696;改革:537;工作:495;加强:485;企业:485;继续:455;国家:435;社会:432;我们:399;我国:378;社会主义:350;积极:340;进一步:334;人民:331;提高:311;政府:289;增加:276;必须:275;\n",
    "\n",
    "【1998-2007】\n",
    "发展:814;建设:597;加强:536;经济:459;工作:430;改革:402;企业:368;继续:320;社会:287;政府:284;推进:261;增加:245;加快:240;积极:240;进一步:236;坚持:228;我们:221;提高:217;农村:207;管理:203;\n",
    "\n",
    "【2008-2017】\n",
    "发展:1115;建设:597;经济:554;推进:507;改革:479;加强:456;社会:345;加快:344;政府:320;提高:312;实施:301;促进:301;我们:294;工作:287;制度:272;增长:259;完善:248;政策:240;就业:240;企业:240;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同时也绘出了5张图，分别如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从以上5张图可以看出，1954~1964年间，“我们”是绝对的关键词，其次第二梯队是：人民，国家，我国，建设，发展，社会主义，第三梯队是：生产，工业，农业，从中可以感受到鲜明的时代气息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "到了1978年改革开放及其后的十年间，“发展”成为了绝对的关键词，而第二梯队的关键词是：经济，建设，我国，企业...“生产”也是提到的次数很多的关键词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1988~1997这十年间，“发展”依然是绝对的关键词，而第二梯队的关键词基本还是：经济，建设，改革，企业...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1998~2007是进入新世纪的十年，“发展”的主旋律依然没有变化，“农村”这一关键词进入前20，体现国家对农业的重视。印象中也就是在这几年间国家取消了延续了2000多年历史的农业税，从此不用再“交公粮”了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再看最近的十年：2008~2017，"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
